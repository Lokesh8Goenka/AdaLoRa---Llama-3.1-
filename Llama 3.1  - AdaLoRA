{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9401942,"sourceType":"datasetVersion","datasetId":5707438},{"sourceId":104449,"sourceType":"modelInstanceVersion","modelInstanceId":68809,"modelId":91102}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ******************************* 0 ******************************* \n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-18T09:42:05.084974Z","iopub.execute_input":"2024-09-18T09:42:05.085369Z","iopub.status.idle":"2024-09-18T09:42:05.491949Z","shell.execute_reply.started":"2024-09-18T09:42:05.085335Z","shell.execute_reply":"2024-09-18T09:42:05.490834Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/llama-3.1/transformers/8b-instruct/2/model.safetensors.index.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00003-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/LICENSE\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00001-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/README.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/USE_POLICY.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00004-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/special_tokens_map.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/.gitattributes\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00002-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/generation_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/consolidated.00.pth\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/params.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/tokenizer.model\n/kaggle/input/combined-data/combines_dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# Necessary until transformers package is updated in the Kaggle notebook environment.\n!pip install --upgrade transformers\n\nimport transformers\nimport torch\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nmodel = \"/kaggle/input/llama-3.1/transformers/8b/1\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline = transformers.pipeline(\n    \"text-generation\", model=model,max_length = 2048, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline('''Generate 15 MCQs from the following passage of different difficulty levels. \nThe questions should be of easy, medium and hard difficulty level, where each difficulty level should have 5 questions each.\nHorticulture Growing vegetables, flowers and fruits for commercial use.\nFarm System Agriculture or farming can be looked at as a system. The important inputs are\nseeds, fertilisers, machinery and labour. Some of the operations involved are ploughing, sowing,\nirrigation, weeding and harvesting. The outputs from the system include crops, wool, dairy and\npoultry products. Types of Farming Farming is practised in various ways across the world.\nDepending upon the geographical conditions, demand of produce, labour and level of\ntechnology, farming can be classified into two main types. These are subsistence farming and\ncommercial farming. Subsistence Farming This type of farming is practised to meet the needs of\nthe farmer’s family. Traditionally, low levels of technology and household labour are used to\nproduce on small output. Subsistence farming can be further classified as intensive subsistence\nand primitive subsistence farming. In intensive subsistence agriculture the farmer cultivates a\nsmall plot of land using simple tools and more labour. Climate with large number of days with\nsunshine and fertile soils permit growing of more than one crop annually on the same plot. Rice\nis the main crop. Other crops include wheat, maize, pulses and oilseeds. Intensive subsistence\nagriculture is prevalent in the thickly populated areas of the monsoon regions of south,\nsoutheast and east Asia. Primitive subsistence agriculture includes shifting cultivation and\nnomadic herding. Shifting cultivation is practised in the thickly forested areas of Amazon basin,\ntropical Africa, parts of southeast Asia and Northeast India. These are the areas of heavy\nrainfall and quick regeneration of vegetation. A plot of land is cleared by felling the trees and\nburning them. The ashes are then mixed with the soil and crops like maize, yam, potatoes and\ncassava are grown. After the soil loses its fertility, the land is abandoned and the cultivator\nmoves to a new plot. Shifting cultivation is also known as ‘slash and burn’ agriculture. Nomadic\nherding is practised in the semi-arid and arid regions of Sahara, Central Asia and some parts of\nIndia, like Rajasthan and Jammu and Kashmir. In this type of farming, herdsmen move from\nplace to place with their animals for fodder and water, along defined routes. This type of\nmovement arises in response to climatic constraints and terrain. Sheep, camel, yak and goatsare most commonly reared. They provide milk, meat, wool, hides and other products to the\nherders and their families.\n''',max_length = 2048)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Llama 3.1 8B-Instruct","metadata":{}},{"cell_type":"code","source":"passage = '''Horticulture Growing vegetables, flowers and fruits for commercial use.\nFarm System Agriculture or farming can be looked at as a system. The important inputs are\nseeds, fertilisers, machinery and labour. Some of the operations involved are ploughing, sowing,\nirrigation, weeding and harvesting. The outputs from the system include crops, wool, dairy and\npoultry products. Types of Farming Farming is practised in various ways across the world.\nDepending upon the geographical conditions, demand of produce, labour and level of\ntechnology, farming can be classified into two main types. These are subsistence farming and\ncommercial farming. Subsistence Farming This type of farming is practised to meet the needs of\nthe farmer’s family. Traditionally, low levels of technology and household labour are used to\nproduce on small output. Subsistence farming can be further classified as intensive subsistence\nand primitive subsistence farming. In intensive subsistence agriculture the farmer cultivates a\nsmall plot of land using simple tools and more labour. Climate with large number of days with\nsunshine and fertile soils permit growing of more than one crop annually on the same plot. Rice\nis the main crop. Other crops include wheat, maize, pulses and oilseeds. Intensive subsistence\nagriculture is prevalent in the thickly populated areas of the monsoon regions of south,\nsoutheast and east Asia. Primitive subsistence agriculture includes shifting cultivation and\nnomadic herding. Shifting cultivation is practised in the thickly forested areas of Amazon basin,\ntropical Africa, parts of southeast Asia and Northeast India. These are the areas of heavy\nrainfall and quick regeneration of vegetation. A plot of land is cleared by felling the trees and\nburning them. The ashes are then mixed with the soil and crops like maize, yam, potatoes and\ncassava are grown. After the soil loses its fertility, the land is abandoned and the cultivator\nmoves to a new plot. Shifting cultivation is also known as ‘slash and burn’ agriculture. Nomadic\nherding is practised in the semi-arid and arid regions of Sahara, Central Asia and some parts of\nIndia, like Rajasthan and Jammu and Kashmir. In this type of farming, herdsmen move from\nplace to place with their animals for fodder and water, along defined routes. This type of\nmovement arises in response to climatic constraints and terrain. Sheep, camel, yak and goatsare most commonly reared. They provide milk, meat, wool, hides and other products to the\nherders and their families.'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Necessary until transformers packages is updated in the Kaggle notebook environment.\n!pip install --upgrade transformers\n\nimport transformers\nimport torch\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nmodel_id = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = '''Today you have to conduct a MCQ based test on a given topic. For the test generate 15 MCQs from the following passage with the answer key.\nThe Test consists of three different sections where each section contains 5 questions each of easy, medium and hard difficulty level.\nUse Bloom's Taxonomy to define the difficulty level of the question.\nRemember the questions should be from the passage only. '''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are an assitant social studies teacher in a High School.\"},\n    {\"role\": \"user\", \"content\": prompt + passage},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=2048,\n)\nprint(outputs[0][\"generated_text\"][-1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generated MCQS on ","metadata":{}},{"cell_type":"markdown","source":"\"**Section 1: Recall (Easy Difficulty Level)**\\n\\n1. What is Horticulture?\\n   a) Growing vegetables, flowers and fruits for commercial use\\n   b) Farming for subsistence\\n   c) Farming for commercial use\\n   d) Growing crops for household use\\n\\nAnswer: a) Growing vegetables, flowers and fruits for commercial use\\n\\n2. What are the important inputs in a farm system?\\n   a) Seeds, fertilisers, machinery and labour\\n   b) Seeds, fertilisers, animals and soil\\n   c) Seeds, labour, machinery and water\\n   d) Seeds, fertilisers, animals and tools\\n\\nAnswer: a) Seeds, fertilisers, machinery and labour\\n\\n3. What are the outputs from a farm system?\\n   a) Crops, wool, dairy and poultry products\\n   b) Crops, seeds, fertilisers and labour\\n   c) Crops, animals, tools and water\\n   d) Crops, animals, seeds and soil\\n\\nAnswer: a) Crops, wool, dairy and poultry products\\n\\n4. What is subsistence farming?\\n   a) Farming to meet the needs of the farmer's family\\n   b) Farming for commercial use\\n   c) Farming for household use\\n   d) Farming for export\\n\\nAnswer: a) Farming to meet the needs of the farmer's family\\n\\n5. What is intensive subsistence agriculture?\\n   a) Farming using simple tools and more labour\\n   b) Farming using complex tools and less labour\\n   c) Farming using animals and household labour\\n   d) Farming using machines and commercial labour\\n\\nAnswer: a) Farming using simple tools and more labour\\n\\n\n**Section 2: Analyze (Medium Difficulty Level)**\\n\\n6. What are the geographical conditions that permit intensive subsistence agriculture? (Bloom's Taxonomy: Analyze)\\n   a) Large number of days with sunshine and fertile soils\\n   b) Heavy rainfall and quick regeneration of vegetation\\n   c) Semi-arid and arid regions\\n   d) Thickly populated areas\\n\\nAnswer: a) Large number of days with sunshine and fertile soils\\n\\n7. What are the main crops grown in intensive subsistence agriculture? (Bloom's Taxonomy: Analyze)\\n   a) Rice, wheat, maize, pulses and oilseeds\\n   b) Maize, yam, potatoes and cassava\\n   c) Sheep, camel, yak and goats\\n   d) Cattle, buffalo and horses\\n\\nAnswer: a) Rice, wheat, maize, pulses and oilseeds\\n\\n8. What is shifting cultivation? (Bloom's Taxonomy: Analyze)\\n   a) Farming using simple tools and more labour\\n   b) Farming using animals and household labour\\n   c) Shifting from one plot of land to another\\n   d) Farming using machines and commercial labour\\n\\nAnswer: c) Shifting from one plot of land to another\\n\\n9. What is nomadic herding? (Bloom's Taxonomy: Analyze)\\n   a) Farming using simple tools and more labour\\n   b) Farming using animals and household labour\\n   c) Moving from place to place with animals for fodder and water\\n   d) Farming using machines and commercial labour\\n\\nAnswer: c) Moving from place to place with animals for fodder and water\\n\\n10. What are the products provided by sheep, camel, yak and goats in nomadic herding? (Bloom's Taxonomy: Analyze)\\n    a) Milk, meat, wool, hides and other products\\n    b) Crops, seeds, fertilisers and labour\\n    c) Crops, animals, tools and water\\n    d) Crops, animals, seeds and soil\\n\\nAnswer: a) Milk, meat, wool, hides and other products\\n\\n\n**Section 3: Evaluate (Hard Difficulty Level)**\\n\\n11. What are the limitations of subsistence farming? (Bloom's Taxonomy: Evaluate)\\n    a) Low levels of technology and household labour\\n    b) High levels of technology and commercial labour\\n    c) Large number of days with sunshine and fertile soils\\n    d) Heavy rainfall and quick regeneration of vegetation\\n\\nAnswer: a) Low levels of technology and household labour\\n\\n12. What are the advantages of intensive subsistence agriculture? (Bloom's Taxonomy: Evaluate)\\n    a) High levels of technology and commercial labour\\n    b) Low levels of technology and household labour\\n    c) Large number of days with sunshine and fertile soils\\n    d) Heavy rainfall and quick regeneration of vegetation\\n\\nAnswer: c) Large number of days with sunshine and fertile soils\\n\\n13. What are the limitations of shifting cultivation? (Bloom's Taxonomy: Evaluate)\\n    a) Soil loses its fertility and is abandoned\\n    b) Soil retains its fertility and is reused\\n    c) Large number of days with sunshine and fertile soils\\n    d) Heavy rainfall and quick regeneration of vegetation\\n\\nAnswer: a) Soil loses its fertility and is abandoned\\n\\n14. What are the advantages of nomadic herding? (Bloom's Taxonomy: Evaluate)\\n    a) High levels of technology and commercial labour\\n    b) Low levels of technology and household labour\\n    c) Moving from place to place with animals for fodder and water\\n    d) Farming using machines and commercial labour\\n\\nAnswer: c) Moving from place to place with animals for fodder and water\\n\\n15. What are the geographical conditions that permit nomadic herding? (Bloom's Taxonomy: Evaluate)\\n    a) Large number of days with sunshine and fertile soils\\n    b) Heavy rainfall and quick regeneration of vegetation\\n    c) Semi-arid and arid regions\\n    d) Thickly populated areas\\n\\nAnswer: c) Semi-arid and arid regions\"}","metadata":{}},{"cell_type":"markdown","source":"# Fine-Tuning","metadata":{}},{"cell_type":"markdown","source":"## Using ADALORA\n","metadata":{}},{"cell_type":"code","source":"# ******************************* 0.1 ******************************* \n!pip install -U bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:42:09.892386Z","iopub.execute_input":"2024-09-18T09:42:09.893416Z","iopub.status.idle":"2024-09-18T09:42:31.297179Z","shell.execute_reply.started":"2024-09-18T09:42:09.893372Z","shell.execute_reply":"2024-09-18T09:42:31.296172Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.3\n","output_type":"stream"}]},{"cell_type":"code","source":"# ******************************* 0.2 ******************************* \nimport bitsandbytes as bnb\nprint(bnb.__version__)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:42:31.299898Z","iopub.execute_input":"2024-09-18T09:42:31.300423Z","iopub.status.idle":"2024-09-18T09:42:37.095404Z","shell.execute_reply.started":"2024-09-18T09:42:31.300362Z","shell.execute_reply":"2024-09-18T09:42:37.094382Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"0.43.3\n","output_type":"stream"}]},{"cell_type":"code","source":"# ******************************* 0.3 ******************************* \n!pip install transformers\n!pip install peft  # This is for Parameter-Efficient Fine-Tuning (PEFT)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:42:37.096892Z","iopub.execute_input":"2024-09-18T09:42:37.097752Z","iopub.status.idle":"2024-09-18T09:43:06.138321Z","shell.execute_reply.started":"2024-09-18T09:42:37.097707Z","shell.execute_reply":"2024-09-18T09:43:06.137242Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nCollecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.12.0\n","output_type":"stream"}]},{"cell_type":"code","source":"model_id = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:43:06.141676Z","iopub.execute_input":"2024-09-18T09:43:06.142385Z","iopub.status.idle":"2024-09-18T09:43:06.147061Z","shell.execute_reply.started":"2024-09-18T09:43:06.142346Z","shell.execute_reply":"2024-09-18T09:43:06.146061Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Necessary until transformers packages is updated in the Kaggle notebook environment.\n!pip install --upgrade transformers\n\nimport transformers\nimport torch\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\n\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = pipeline.model","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adalora\n\nfrom peft import AdaLoraConfig, AdaLoraModel\n\nadalora_config = AdaLoraConfig(\n    task_type=\"CAUSAL_LM\",\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1\n)\n\nmodel = AdaLoraModel(model, adalora_config, \"default\")\n\n# number of trainable parameters\nnum_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n#total number of parameters\ntotal_params = sum(p.numel() for p in model.parameters())\n\n# percentage of trainable parameters\npercentage_trainable_params = (num_trainable_params / total_params) * 100\n\nprint(\"Number of trainable parameters:\", num_trainable_params)\nprint(\"Total number of parameters:\", total_params)\nprint(\"Percentage of trainable parameters:\", percentage_trainable_params)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# loading dataset","metadata":{}},{"cell_type":"code","source":"# ******************************* 1 ******************************* \nimport pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/combined-data/combines_dataset.csv', encoding='ISO-8859-1')\n\n# Extract passages and mcqs using column names\n# passages = df['passage'].tolist()\n# mcqs = df['mcqs'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:43:06.148224Z","iopub.execute_input":"2024-09-18T09:43:06.148565Z","iopub.status.idle":"2024-09-18T09:43:06.251605Z","shell.execute_reply.started":"2024-09-18T09:43:06.148519Z","shell.execute_reply":"2024-09-18T09:43:06.250807Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# ******************************* 1.1 ******************************* \n# Display the first few rows\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:43:06.252724Z","iopub.execute_input":"2024-09-18T09:43:06.253026Z","iopub.status.idle":"2024-09-18T09:43:06.279149Z","shell.execute_reply.started":"2024-09-18T09:43:06.252994Z","shell.execute_reply":"2024-09-18T09:43:06.278081Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                             passage  \\\n0  This transformation from a plant to a finished...   \n1  People are a nationâs greatest resource. Nat...   \n2  Have you ever given a thought to the fact that...   \n3  In a small village in Tanzania, Africa, Mamba ...   \n4  Water, electricity, rickshaw, vegetable and te...   \n\n                                                mcqs  \n0  ['## Agriculture and Economic Activities: An M...  \n1  ['## Population Studies Test\\n', '\\n', '**Inst...  \n2  ['## The Journey of Your Notebook: A Social St...  \n3  ['## Social Studies Test: Land as a Resource\\n...  \n4  ['## Agriculture and Farming: An MCQ Test\\n', ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>passage</th>\n      <th>mcqs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>This transformation from a plant to a finished...</td>\n      <td>['## Agriculture and Economic Activities: An M...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>People are a nationâs greatest resource. Nat...</td>\n      <td>['## Population Studies Test\\n', '\\n', '**Inst...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Have you ever given a thought to the fact that...</td>\n      <td>['## The Journey of Your Notebook: A Social St...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>In a small village in Tanzania, Africa, Mamba ...</td>\n      <td>['## Social Studies Test: Land as a Resource\\n...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Water, electricity, rickshaw, vegetable and te...</td>\n      <td>['## Agriculture and Farming: An MCQ Test\\n', ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# ******************************* 1.3 ******************************* \n# Import the splitting function\nfrom sklearn.model_selection import train_test_split\n\n# Split the formatted data into training and testing sets\ntrain_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n\n# Check the lengths of the train and test splits\nprint(f\"Training set size: {len(train_data)}\")\nprint(f\"Testing set size: {len(test_data)}\")\n\n# Optional: Print the first entry from the training and test set\nprint(f\"First training example: {train_data.iloc[0]}\")\nprint(len(train_data))\nprint(f\"First testing example: {test_data.iloc[0]}\")\nprint(len(test_data))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:43:06.280316Z","iopub.execute_input":"2024-09-18T09:43:06.280647Z","iopub.status.idle":"2024-09-18T09:43:07.021827Z","shell.execute_reply.started":"2024-09-18T09:43:06.280613Z","shell.execute_reply":"2024-09-18T09:43:07.020722Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Training set size: 114\nTesting set size: 29\nFirst training example: passage    1. Peasants and Agricultural Production The ba...\nmcqs       ['## Peasant Life and Agriculture in Mughal In...\nName: 124, dtype: object\n114\nFirst testing example: passage    1. A Mosaic of Religious Beliefs and Practices...\nmcqs       ['## MCQ Test: A Mosaic of Religious Beliefs\\n...\nName: 117, dtype: object\n29\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data = train_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:43:07.023327Z","iopub.execute_input":"2024-09-18T09:43:07.023984Z","iopub.status.idle":"2024-09-18T09:43:07.030088Z","shell.execute_reply.started":"2024-09-18T09:43:07.023928Z","shell.execute_reply":"2024-09-18T09:43:07.028849Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"passages = train_data['passage'].tolist()\nmcqs = train_data['mcqs'].tolist()\n\npassage_test = test_data['passage'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:43:07.031360Z","iopub.execute_input":"2024-09-18T09:43:07.032180Z","iopub.status.idle":"2024-09-18T09:43:07.044753Z","shell.execute_reply.started":"2024-09-18T09:43:07.032144Z","shell.execute_reply":"2024-09-18T09:43:07.043905Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# ******************************* 1.2 ******************************* \nformatted_data = []\n\nfor passage, mcq in zip(passages, mcqs):\n    formatted_entry = (\n        \"<begin_of_text> \"\n        \"<start_header_id>passage<end_header_id> \" + passage.strip() + \" \"\n        \"<eom_id> \"\n        \"<start_header_id>questions<end_header_id> \" + mcq.strip() + \" \"\n        \"<eot_id> \"\n        \"<end_of_text>\"\n    )\n    formatted_data.append(formatted_entry)\n\nprint(formatted_data[72])","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:43:07.047264Z","iopub.execute_input":"2024-09-18T09:43:07.047607Z","iopub.status.idle":"2024-09-18T09:43:07.061626Z","shell.execute_reply.started":"2024-09-18T09:43:07.047575Z","shell.execute_reply":"2024-09-18T09:43:07.060683Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"<begin_of_text> <start_header_id>passage<end_header_id> The Non-Cooperation-Khilafat Movement began in January 1921. Various social groups\nparticipated in this movement, each with its own specific aspiration. All of them responded to the\ncall of Swaraj, but the term meant different things to different people. The movement started\nwith middle-class participation in the cities. Thousands of students left government-controlled\nschools and colleges, headmasters and teachers resigned, and lawyers gave up their legal\npractices. The council elections were boycotted in most provinces except Madras, where the\nJustice Party, the party of the non-Brahmans, felt that entering the council was one way of\ngaining some power â something that usually only Brahmans had access to. The effects of\nnon-cooperation on the economic front were more dramatic. Foreign goods were boycotted,\nliquor shops picketed, and foreign cloth burnt in huge bonfires. The import of foreign cloth\nhalved between 1921 and 1922, its value dropping from Rs 102 crore to Rs 57 crore. In many\nplaces merchants and traders refused to trade in foreign goods or finance foreign trade. As the\nboycott movement spread, and people began discarding imported clothes and wearing only\nIndian ones, production of Indian textile mills and handlooms went up. But this movement in the\ncities gradually slowed down for a variety of reasons. Khadi cloth was often more expensive\nthan massproduced mill cloth and poor people could not afford to buy it. How then could they\nboycott mill cloth for too long? Similarly the boycott of British institutions posed a problem. For\nthe movement to be successful, alternative Indian institutions had to be set up so that they could\nbe used in place of the British ones. These were slow to come up. So students and teachers\nbegan trickling back to government schools and lawyers joined back work in government courts.\nFrom the cities, the Non-Cooperation Movement spread to the countryside. It drew into its fold\nthe struggles of peasants and tribals which were developing in different parts of India in the\nyears after the war. In Awadh, peasants were led by Baba Ramchandra â a sanyasi who had\nearlier been to Fiji as an indentured labourer. The movement here was against talukdars and\nlandlords who demanded from peasants exorbitantly high rents and a variety of other cesses.\nPeasants had to do begar and work at landlordsâ farms without any payment. As tenants they\nhad no security of tenure, being regularly evicted so that they could acquire no right over the\nleased land. The peasant movement demanded reduction of revenue, abolition of begar, and\nsocial boycott of oppressive landlords. In many places nai â dhobi bandhs were organised by\npanchayats to deprive landlords of the services of even barbers and washermen. In June 1920,\nJawaharlal Nehru began going around the villages in Awadh, talking to the villagers, and trying\nto understand their grievances. By October, the Oudh Kisan Sabha was set up headed by\nJawaharlal Nehru, Baba Ramchandra and a few others. Within a month, over 300 branches had\nbeen set up in the villages around the region. So when the NonCooperation Movement began\nthe following year, the effort of the Congress was to integrate the Awadh peasant struggle into\nthe wider struggle. The peasant movement, however, developed in forms that the Congress\nleadership was unhappy with. As the movement spread in 1921, the houses of talukdars and\nmerchants were attacked, bazaars were looted, and grain hoards were taken over. In many\nplaces local leaders told peasants that Gandhiji had declared that no taxes were to be paid and\nland was to be redistributed among the poor. The name of the Mahatma was being invoked to\nsanction all action and aspirations. Tribal peasants interpreted the message of Mahatma Gandhi\nand the idea of swaraj in yet another way. In the Gudem Hills of Andhra Pradesh, for instance, a\nmilitant guerrilla movement spread in the early 1920s â not a form of struggle that the Congress\ncould approve. Here, as in other forest regions, the colonial government had closed large forestareas, preventing people from entering the forests to graze their cattle, or to collect fuelwood\nand fruits. This enraged the hill people. Not only were their livelihoods affected but they felt that\ntheir traditional rights were being denied. When the government began forcing them to\ncontribute begar for road building, the hill people revolted. The person who came to lead them\nwas an interesting figure. Alluri Sitaram Raju claimed that he had a variety of special powers: he\ncould make correct astrological predictions and heal people, and he could survive even bullet\nshots. Captivated by Raju, the rebels proclaimed that he was an incarnation of God. Raju talked\nof the greatness of Mahatma Gandhi, said he was inspired by the Non-Cooperation Movement,\nand persuaded people to wear khadi and give up drinking. But at the same time he asserted\nthat India could be liberated only by the use of force, not non-violence. The Gudem rebels\nattacked police stations, attempted to kill British officials and carried on guerrilla warfare for\nachieving swaraj. Raju was captured and executed in 1924, and over time became a folk hero.\nWorkers too had their own understanding of Mahatma Gandhi and the notion of swaraj. For\nplantation workers in Assam, freedom meant the right to move freely in and out of the confined\nspace in which they were enclosed, and it meant retaining a link with the village from which they\nhad come. Under the Inland Emigration Act of 1859, plantation workers were not permitted to\nleave the tea gardens without permission, and in fact they were rarely given such permission.\nWhen they heard of the Non-Cooperation Movement, thousands of workers defied the\nauthorities, left the plantations and headed home. They believed that Gandhi Raj was coming\nand everyone would be given land in their own villages. They, however, never reached their\ndestination. Stranded on the way by a railway and steamer strike, they were caught by the\npolice and brutally beaten up. The visions of these movements were not defined by the\nCongress programme. They interpreted the term swaraj in their own ways, imagining it to be a\ntime when all suffering and all troubles would be over. Yet, when the tribals chanted Gandhijiâs\nname and raised slogans demanding âSwatantra Bharatâ, they were also emotionally relating to\nan all-India agitation. When they acted in the name of Mahatma Gandhi, or linked their\nmovement to that of the Congress, they were identifying with a movement which went beyond\nthe limits of their immediate <eom_id> <start_header_id>questions<end_header_id> ['## The Non-Cooperation Movement: A Test of Understanding\\n', '\\n', '**Easy**\\n', '\\n', 'Q1: When did the Non-Cooperation-Khilafat Movement begin?\\n', 'A) January 1920\\n', 'B) January 1921\\n', 'C) January 1922\\n', 'D) January 1923\\n', '\\n', \"Q2: Which of these professions saw people resigning en masse during the movement's initial urban phase?\\n\", 'A) Doctors\\n', 'B) Lawyers\\n', 'C) Merchants\\n', 'D) Farmers\\n', '\\n', 'Q3: What happened to the import of foreign cloth between 1921 and 1922 due to the boycott?\\n', 'A) It halved\\n', 'B) It doubled\\n', 'C) It remained unchanged\\n', 'D) It increased by 50%\\n', '\\n', 'Q4: Who led the peasant movement in Awadh?\\n', 'A) Baba Ramchandra\\n', 'B) Jawaharlal Nehru\\n', 'C) Mahatma Gandhi\\n', 'D) Alluri Sitaram Raju\\n', '\\n', 'Q5: Alluri Sitaram Raju led a movement in which region?\\n', 'A) Gudem Hills\\n', 'B) Awadh\\n', 'C) Madras\\n', 'D) Fiji\\n', '\\n', '**Medium**\\n', '\\n', 'Q6: Why did the Congress leadership disapprove of some forms of peasant protest?\\n', 'A) They were non-violent and ineffective\\n', 'B) They targeted only British officials\\n', \"C) They involved looting and attacks, going against the Congress' principles\\n\", 'D) They focused solely on land redistribution demands\\n', '\\n', 'Q7: What was the primary demand of plantation workers in Assam?\\n', 'A) Higher wages\\n', 'B) Freedom of movement and connection to their villages\\n', 'C) Better working conditions\\n', 'D) Recognition of their trade union\\n', '\\n', 'Q8: What grievance did the Gudem Hill people have against the colonial government?\\n', 'A) Forced recruitment into the army\\n', 'B) Restriction of access to forest resources\\n', 'C) Imposition of new taxes on agricultural produce\\n', 'D) Ban on religious practices\\n', '\\n', 'Q9: How did many peasants and tribals understand \"swaraj\"?\\n', 'A) As a time of complete liberation from all troubles\\n', 'B) As a system with elected Indian representatives\\n', 'C) As a return to traditional village life\\n', 'D) As an opportunity to gain education and Western technology\\n', '\\n', \"Q10: What was the Justice Party's reason for participating in council elections, unlike most other groups?\\n\", 'A) They believed in achieving independence through negotiations\\n', 'B) They wanted to use the council to gain power previously inaccessible to non-Brahmans\\n', 'C) They aimed to prove the effectiveness of British-designed institutions\\n', 'D) They saw it as a way to gather support for armed rebellion\\n', '\\n', '**Hard**\\n', '\\n', 'Q11: What was the significance of the \"nai-dhobi bandhs\" organized by village panchayats?\\n', 'A) They aimed to cripple the local economy and force British intervention\\n', 'B) They were a form of non-violent protest targeting landlords by depriving them of essential services\\n', 'C) They were religious rituals meant to bring about divine intervention\\n', 'D) They were attempts to establish parallel government structures in villages\\n', '\\n', \"Q12: What distinguished Alluri Sitaram Raju's movement from the mainstream Congress approach?\\n\", 'A) He rejected the use of khadi and traditional symbols\\n', 'B) He believed in achieving independence through armed struggle\\n', 'C) He focused solely on the grievances of the wealthy elite\\n', 'D) He collaborated directly with the British authorities\\n', '\\n', 'Q13: What was the ultimate fate of Alluri Sitaram Raju?\\n', 'A) He was elected as a representative to the Indian government\\n', 'B) He successfully negotiated a peace treaty with the British\\n', 'C) He escaped British capture and lived in hiding\\n', 'D) He was captured and executed by the British\\n', '\\n', 'Q14: What does the variety of interpretations of \"swaraj\" during the movement reveal?\\n', 'A) The lack of a cohesive national identity among Indians\\n', 'B) The success of the British in dividing the population\\n', 'C) The diverse and often localized understanding of freedom and its goals\\n', 'D) The inherent weakness of non-violent movements\\n', '\\n', 'Q15: What was the common thread linking the diverse groups participating in the Non-Cooperation-Khilafat Movement?\\n', 'A) A shared belief in the superiority of Western civilization\\n', 'B) A desire to return to pre-colonial societal structures\\n', \"C) A rejection of Gandhi's leadership and principles\\n\", 'D) A yearning for \"swaraj,\" even if its meaning differed across groups\\n', '\\n', '**Answer Key**\\n', '\\n', '**Easy**\\n', '1. B\\n', '2. B\\n', '3. A\\n', '4. A\\n', '5. A\\n', '\\n', '**Medium**\\n', '6. C\\n', '7. B\\n', '8. B\\n', '9. A\\n', '10. B\\n', '\\n', '**Hard**\\n', '11. B\\n', '12. B\\n', '13. D\\n', '14. C\\n', '15. D \\n'] <eot_id> <end_of_text>\n","output_type":"stream"}]},{"cell_type":"code","source":"# ******************************* 1.3 ******************************* \nevaluation_data = []\n\nfor passage in (passage_test):\n    formatted_entry = (\n        \"<begin_of_text> \"\n        \"<start_header_id>passage<end_header_id> \" + passage.strip() + \" \"\n        \"<eom_id> \"\n        \"<end_of_text>\"\n    )\n    evaluation_data.append(formatted_entry)\n\nprint(evaluation_data[72])","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:43:07.062765Z","iopub.execute_input":"2024-09-18T09:43:07.063133Z","iopub.status.idle":"2024-09-18T09:43:07.162847Z","shell.execute_reply.started":"2024-09-18T09:43:07.063088Z","shell.execute_reply":"2024-09-18T09:43:07.161532Z"},"trusted":true},"execution_count":13,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m     formatted_entry \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<begin_of_text> \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<start_header_id>passage<end_header_id> \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m passage\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<eom_id> \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<end_of_text>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     11\u001b[0m     evaluation_data\u001b[38;5;241m.\u001b[39mappend(formatted_entry)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mevaluation_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m72\u001b[39;49m\u001b[43m]\u001b[49m)\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"],"ename":"IndexError","evalue":"list index out of range","output_type":"error"}]},{"cell_type":"code","source":"# ******************************* 2 ******************************* \nfrom transformers import LlamaForCausalLM\nfrom peft import get_peft_model, LoraConfig\n\n# Load the model with quantization\nmodel = LlamaForCausalLM.from_pretrained(model_id, load_in_4bit=True, device_map='auto')\n\n# Apply LoRA\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"v_proj\"]  \n)\n\n# Wrap the model with QLoRA\nmodel = get_peft_model(model, lora_config)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:43:54.315230Z","iopub.execute_input":"2024-09-18T09:43:54.315661Z","iopub.status.idle":"2024-09-18T09:45:44.350314Z","shell.execute_reply.started":"2024-09-18T09:43:54.315624Z","shell.execute_reply":"2024-09-18T09:45:44.349307Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a65156d704bd43f9974095205940f728"}},"metadata":{}}]},{"cell_type":"code","source":"# ******************************* 2.1 ******************************* \nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/llama-3.1/transformers/8b-instruct/2/\")\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:45:44.352554Z","iopub.execute_input":"2024-09-18T09:45:44.353266Z","iopub.status.idle":"2024-09-18T09:45:44.849441Z","shell.execute_reply.started":"2024-09-18T09:45:44.353211Z","shell.execute_reply":"2024-09-18T09:45:44.848527Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"# ******************************* 3 ******************************* \nfrom torch.utils.data import Dataset\n\nclass TextGenerationDataset(Dataset):\n    def __init__(self, formatted_data, tokenizer, max_length=1024):\n        self.formatted_data = formatted_data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.formatted_data)\n\n    def __getitem__(self, idx):\n        # Get the formatted entry\n        formatted_entry = self.formatted_data.iloc[idx]\n        \n        # Tokenize the entire formatted entry\n        encoding = self.tokenizer(\n            formatted_entry,\n            return_tensors='pt',\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True\n        )\n\n        # Extract input_ids and create labels\n        input_ids = encoding['input_ids'].squeeze()\n        labels = input_ids.clone()  # Labels should match input_ids for causal LM\n\n        return {'input_ids': input_ids, 'labels': labels}\n\n# Prepare the dataset using the new formatted data\ntrain_dataset = TextGenerationDataset(formatted_data, tokenizer)\ntest_dataset =  TextGenerationDataset(evaluation_data, tokenizer)\n\n# Define the DataLoader if needed\n# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)  # Adjust batch size as needed\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:45:44.850688Z","iopub.execute_input":"2024-09-18T09:45:44.850998Z","iopub.status.idle":"2024-09-18T09:45:44.859330Z","shell.execute_reply.started":"2024-09-18T09:45:44.850966Z","shell.execute_reply":"2024-09-18T09:45:44.858345Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# ******************************* 3.1 ******************************* \nimport torch\nprint(torch.cuda.device_count())  ","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:45:44.861087Z","iopub.execute_input":"2024-09-18T09:45:44.861367Z","iopub.status.idle":"2024-09-18T09:45:44.878178Z","shell.execute_reply.started":"2024-09-18T09:45:44.861336Z","shell.execute_reply":"2024-09-18T09:45:44.877306Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"2\n","output_type":"stream"}]},{"cell_type":"code","source":"# ******************************* 3.2 ******************************* \nimport torch\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:45:44.879337Z","iopub.execute_input":"2024-09-18T09:45:44.879655Z","iopub.status.idle":"2024-09-18T09:45:44.891749Z","shell.execute_reply.started":"2024-09-18T09:45:44.879624Z","shell.execute_reply":"2024-09-18T09:45:44.891020Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# ******************************* 4 ******************************* \nfrom transformers import TrainingArguments, Trainer\n\n# Define training arguments for multi-GPU and FP16\ntraining_args = TrainingArguments(\n    output_dir='./results_1',\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    save_steps=100,\n    save_total_limit=1,\n    logging_dir='./logs',\n    report_to='none',  # optional: to avoid logging issues with multiple GPUs\n    ddp_find_unused_parameters=True,  # required for multi-GPU\n    fp16=True,  \n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=test_dataset\n    )\n\n# Fine-tune the model\ntrainer.train()\n\neval_results = trainer.evaluate(eval_dataset=eval_dataset)\nprint(f\"BLEU score: {eval_results['eval_bleu']}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:45:44.892740Z","iopub.execute_input":"2024-09-18T09:45:44.893046Z","iopub.status.idle":"2024-09-18T09:46:09.990031Z","shell.execute_reply.started":"2024-09-18T09:45:44.893014Z","shell.execute_reply":"2024-09-18T09:46:09.988542Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 72","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 26\u001b[0m\n\u001b[1;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     20\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     21\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_data,\n\u001b[1;32m     22\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_bleu\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1948\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1946\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1949\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2246\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2243\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2245\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2247\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n","\u001b[0;31mKeyError\u001b[0m: 72"],"ename":"KeyError","evalue":"72","output_type":"error"}]},{"cell_type":"code","source":"print(train_data.index)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:37:40.638626Z","iopub.execute_input":"2024-09-18T10:37:40.639105Z","iopub.status.idle":"2024-09-18T10:37:40.961174Z","shell.execute_reply.started":"2024-09-18T10:37:40.639065Z","shell.execute_reply":"2024-09-18T10:37:40.960141Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain_data\u001b[49m\u001b[38;5;241m.\u001b[39mindex)\n","\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"],"ename":"NameError","evalue":"name 'train_data' is not defined","output_type":"error"}]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    save_steps=10_000,\n    save_total_limit=2,\n    logging_dir='./logs',\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\n# Fine-tune the model\ntrainer.train()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import LlamaTokenizer\n\n# Load the tokenizer\ntokenizer = LlamaTokenizer.from_pretrained('path/to/llama-3.1')\n\n# Tokenize each formatted entry\ntokenized_data = [tokenizer(entry, return_tensors='pt', padding='max_length', truncation=True) for entry in formatted_data]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pre-processing\n\ndef preprocess_data(row):\n    passage = row['passage']\n    mcqs = row['mcqs']\n    # Combine the passage and questions into a single text\n    combined_text = f\"{passage}\\n\\n{mcqs}\"\n    return combined_text\n\n# Apply the preprocessing function\ndf['combined_text'] = df.apply(preprocess_data, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df['combined_text'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = pipeline.tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    tokenizer.pad_token = '[PAD]'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert your DataFrame to a Hugging Face Dataset\ndataset = Dataset.from_pandas(df)\n\n# Define your tokenize_function\ndef tokenize_function(examples):\n    inputs = tokenizer(examples[\"combined_text\"], padding=\"max_length\", truncation=True, max_length=512)\n    return inputs\n\n# Apply the tokenize_function to the dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the dataset into training and validation sets\nsplit_dataset = tokenized_dataset.train_test_split(test_size=0.2)\ntrain_dataset = split_dataset['train']\nval_dataset = split_dataset['test']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset, load_metric","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer, AutoTokenizer, DataCollatorForSeq2Seq\n\n# Data Collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=pipeline.model)\n\n# Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    report_to=None,\n    eval_strategy=\"steps\",\n    learning_rate=1e-4,\n    per_device_train_batch_size=1,\n    num_train_epochs=50,\n    weight_decay=0.01,\n    logging_steps=10,\n    fp16=True,\n    gradient_checkpointing=True,\n)\n\n# Trainer\ntrainer = Trainer(\n    model=pipeline.model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# Training\ntrainer.train()\n\n# Evaluate\neval_results = trainer.evaluate()\nprint(f\"BLEU score: {eval_results['eval_bleu']}\")\n\n# Save the model\n# pipeline.model.save_pretrained(\"./fine-tuned-llama\")\n# tokenizer.save_pretrained(\"./fine-tuned-llama\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}